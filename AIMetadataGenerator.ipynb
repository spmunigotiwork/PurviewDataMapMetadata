{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e22b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f \n",
    "{\n",
    "  \"defaultLakehouse\": {\n",
    "    \"name\": \"<Lakehouse-Name>\",\n",
    "    \"id\": \"<Lakehouse-ID>\",\n",
    "    \"workspaceId\": \"<Workspace-ID>\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "\n",
    "# OUTPUT Configuration (where to save results)\n",
    "output_workspace_id = '<WorkspaceId>'  # Change this to your output workspace ID\n",
    "output_lakehouse_id = '<LakehouseId>'  # Change this to your output lakehouse ID\n",
    "output_table_name = \"<OutputTableName>\"  # Updated table name to reflect attached approach\n",
    "output_csv_full_path = \"abfss://<>@onelake.dfs.fabric.microsoft.com/<LHName>.Lakehouse/Files/<Name>.csv\" # Change this to your desired output CSV path in Fabric LH\n",
    "\n",
    "# Configuration - Modify these variables to change processing behavior\n",
    "# tables_to_process = \"ALL\"  # Can be: \"table_name\", [\"table1\", \"table2\"], or \"ALL\"\n",
    "tables_to_process = [\"All\"]  # Can be: \"table_name\", [\"table1\", \"table2\"], or \"ALL\"\n",
    "tables_to_skip = []  # List of tables to skip: [\"temp_table\", \"staging_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attached lakehouse information\n",
    "try:\n",
    "    # Get current lakehouse info from Spark catalog\n",
    "    current_db = spark.catalog.currentDatabase()\n",
    "    print(f\"Current database/lakehouse: {current_db}\")\n",
    "    \n",
    "    # Extract workspace and lakehouse IDs from the configuration cell above\n",
    "    # These values come from the %%configure cell\n",
    "    input_workspace_id = notebookutils.runtime.context.get('defaultLakehouseWorkspaceId')\n",
    "    input_lakehouse_id = notebookutils.runtime.context.get('defaultLakehouseId')\n",
    "    \n",
    "    print(f\"Using attached lakehouse:\")\n",
    "    print(f\"  Workspace ID: {input_workspace_id}\")\n",
    "    print(f\"  Lakehouse ID: {input_lakehouse_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting attached lakehouse info: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5861ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions required for the metadata generator\n",
    "def get_workspace_and_lakehouse_names(workspace_id, lakehouse_id):\n",
    "    \"\"\"Get workspace and lakehouse names from their IDs using Fabric REST APIs\"\"\"\n",
    "    try:\n",
    "        auth_token = notebookutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {auth_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        workspace_name = \"Unknown_Workspace\"\n",
    "        try:\n",
    "            workspace_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}\"\n",
    "            workspace_response = requests.get(workspace_url, headers=headers)\n",
    "            if workspace_response.status_code == 200:\n",
    "                workspace_data = workspace_response.json()\n",
    "                workspace_name = workspace_data.get('displayName', f\"Workspace_{workspace_id[:8]}\")\n",
    "                print(f\"  Retrieved workspace name: {workspace_name}\")\n",
    "            else:\n",
    "                print(f\"  Warning: Could not get workspace name (status: {workspace_response.status_code})\")\n",
    "                workspace_name = f\"Workspace_{workspace_id[:8]}\"\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error getting workspace name: {e}\")\n",
    "            workspace_name = f\"Workspace_{workspace_id[:8]}\"\n",
    "        lakehouse_name = \"Unknown_Lakehouse\"\n",
    "        try:\n",
    "            lakehouse_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\"\n",
    "            lakehouse_response = requests.get(lakehouse_url, headers=headers)\n",
    "            if lakehouse_response.status_code == 200:\n",
    "                lakehouse_data = lakehouse_response.json()\n",
    "                lakehouse_name = lakehouse_data.get('displayName', f\"Lakehouse_{lakehouse_id[:8]}\")\n",
    "                print(f\"  Retrieved lakehouse name: {lakehouse_name}\")\n",
    "            else:\n",
    "                print(f\"  Warning: Could not get lakehouse name (status: {lakehouse_response.status_code})\")\n",
    "                lakehouse_name = f\"Lakehouse_{lakehouse_id[:8]}\"\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error getting lakehouse name: {e}\")\n",
    "            lakehouse_name = f\"Lakehouse_{lakehouse_id[:8]}\"\n",
    "        return workspace_name, lakehouse_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting workspace/lakehouse names via API: {e}\")\n",
    "        workspace_name = f\"Workspace_{workspace_id[:8]}\"\n",
    "        lakehouse_name = f\"Lakehouse_{lakehouse_id[:8]}\"\n",
    "        return workspace_name, lakehouse_name\n",
    "\n",
    "def get_current_timestamp_ist():\n",
    "    from datetime import datetime, timezone, timedelta\n",
    "    ist = timezone(timedelta(hours=5, minutes=30))\n",
    "    return datetime.now(ist).strftime('%Y-%m-%d %H:%M:%S IST')\n",
    "\n",
    "def get_column_datatypes(df):\n",
    "    try:\n",
    "        return {field.name: str(field.dataType) for field in df.schema.fields}\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not get column datatypes: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_efficient_sample(df, table_name, min_sample_size=500, max_sample_size=1000):\n",
    "    try:\n",
    "        row_count = df.count()\n",
    "        print(f\"  Table {table_name} has {row_count:,} rows\")\n",
    "        if row_count == 0:\n",
    "            print(f\"  Warning: Table {table_name} is empty\")\n",
    "            return df.limit(0)\n",
    "        # Guarantee target_sample_size is at least min_sample_size\n",
    "        target_sample_size = max(min_sample_size, min(max_sample_size, int(row_count * 0.1)))\n",
    "        if row_count <= min_sample_size:\n",
    "            sample_df = df\n",
    "            print(f\"  Table is small ({row_count} rows), taking all data for analysis\")\n",
    "        elif row_count <= max_sample_size:\n",
    "            sample_df = df\n",
    "            print(f\"  Table is medium-sized ({row_count} rows), taking all data for comprehensive analysis\")\n",
    "        else:\n",
    "            fraction = max(target_sample_size / row_count, min_sample_size / row_count)\n",
    "            fraction = min(fraction, 1.0)\n",
    "            print(f\"  Large table detected - using extensive randomized sampling\")\n",
    "            print(f\"  Target sample size: {target_sample_size:,} rows (fraction: {fraction:.4f})\")\n",
    "            samples = []\n",
    "            for seed in [42, 123, 456, 789]:\n",
    "                try:\n",
    "                    seed_sample = df.sample(fraction=fraction/4, seed=seed).limit(target_sample_size//4)\n",
    "                    samples.append(seed_sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Seed {seed} sampling failed: {e}\")\n",
    "                    continue\n",
    "            try:\n",
    "                date_cols = [field.name for field in df.schema.fields if 'timestamp' in str(field.dataType).lower() or 'date' in str(field.dataType).lower()]\n",
    "                if date_cols and samples:\n",
    "                    print(f\"    Found date/time columns: {date_cols[:2]} - ensuring temporal diversity\")\n",
    "                    date_col = date_cols[0]\n",
    "                    temporal_sample = df.orderBy(date_col).sample(fraction=fraction/2, seed=999).limit(target_sample_size//2)\n",
    "                    samples.append(temporal_sample)\n",
    "            except Exception as e:\n",
    "                print(f\"    Note: Temporal sampling not available: {e}\")\n",
    "            if samples:\n",
    "                sample_df = samples[0]\n",
    "                for s in samples[1:]:\n",
    "                    sample_df = sample_df.union(s)\n",
    "                id_cols = [field.name for field in sample_df.schema.fields if 'id' in field.name.lower()]\n",
    "                if id_cols:\n",
    "                    sample_df = sample_df.dropDuplicates([id_cols[0]])\n",
    "                sample_df = sample_df.limit(max_sample_size)\n",
    "            else:\n",
    "                print(f\"    Using fallback random sampling\")\n",
    "                sample_df = df.sample(fraction=fraction, seed=42).limit(target_sample_size)\n",
    "        final_sample_size = sample_df.count()\n",
    "        coverage_percent = (final_sample_size / row_count) * 100\n",
    "        print(f\"  ✓ Extensive sampling complete: {final_sample_size:,} rows ({coverage_percent:.2f}% coverage)\")\n",
    "        # Guarantee minimum target_sample_size if possible\n",
    "        if final_sample_size < target_sample_size and row_count > target_sample_size:\n",
    "            print(f\"  ⚠ Sample size {final_sample_size} below target {target_sample_size}, taking random {target_sample_size} rows as fallback\")\n",
    "            sample_df = df.sample(fraction=target_sample_size/row_count, seed=9999).limit(target_sample_size)\n",
    "            final_sample_size = sample_df.count()\n",
    "            print(f\"  ✓ Fallback sample size: {final_sample_size}\")\n",
    "        if final_sample_size >= target_sample_size or final_sample_size == row_count:\n",
    "            print(f\"  ✓ Sample meets target size requirement ({target_sample_size} records)\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Warning: Sample size {final_sample_size} below recommended target of {target_sample_size}\")\n",
    "        return sample_df\n",
    "    except Exception as e:\n",
    "        print(f\"  Error sampling table {table_name}: {e}\")\n",
    "        print(f\"  Attempting basic fallback sampling...\")\n",
    "        try:\n",
    "            basic_sample = df.limit(min_sample_size)\n",
    "            print(f\"  Fallback successful: {basic_sample.count()} rows\")\n",
    "            return basic_sample\n",
    "        except Exception as e2:\n",
    "            print(f\"  Fallback also failed: {e2}\")\n",
    "            return df.limit(0)\n",
    "\n",
    "def get_column_samples(sample_df, max_samples=8):\n",
    "    column_samples = {}\n",
    "    columns = sample_df.columns\n",
    "    for col in columns:\n",
    "        try:\n",
    "            non_null_values = sample_df.select(col).where(f\"{col} IS NOT NULL\").distinct().limit(max_samples * 2).collect()\n",
    "            values = [row[col] for row in non_null_values if row[col] is not None]\n",
    "            if not values:\n",
    "                column_samples[col] = [\"(all null values)\"]\n",
    "                continue\n",
    "            sample_values = []\n",
    "            sorted_vals = sorted(values)\n",
    "            n = len(sorted_vals)\n",
    "            indices = [0, n//4, n//2, 3*n//4, n-1]\n",
    "            for i in indices:\n",
    "                if i < n:\n",
    "                    sample_values.append(str(sorted_vals[i]))\n",
    "            remaining_slots = max_samples - len(sample_values)\n",
    "            if remaining_slots > 0:\n",
    "                import random\n",
    "                random_samples = random.sample(values, min(remaining_slots, len(values)))\n",
    "                sample_values.extend([str(val) for val in random_samples])\n",
    "            seen = set()\n",
    "            unique_samples = []\n",
    "            for val in sample_values:\n",
    "                if val not in seen:\n",
    "                    seen.add(val)\n",
    "                    unique_samples.append(val)\n",
    "            column_samples[col] = unique_samples[:max_samples]\n",
    "        except Exception as e:\n",
    "            column_samples[col] = [f\"(error: {str(e)[:50]})\"]\n",
    "    return column_samples\n",
    "\n",
    "# Global variable to cache table metadata\n",
    "table_metadata_cache = {}\n",
    "\n",
    "def get_all_tables_in_lakehouse_attached():\n",
    "    global table_metadata_cache\n",
    "    table_metadata_cache = {}\n",
    "    try:\n",
    "        print(\"Discovering tables using Spark catalog...\")\n",
    "        tables = spark.catalog.listTables()\n",
    "        table_names = []\n",
    "        print(f\"Found {len(tables)} tables in Spark catalog\")\n",
    "        for table in tables:\n",
    "            table_name = table.name\n",
    "            table_names.append(table_name)\n",
    "            table_metadata_cache[table_name] = {\n",
    "                'catalog': table.catalog,\n",
    "                'namespace': '.'.join(table.namespace) if table.namespace else '',\n",
    "                'tableType': table.tableType,\n",
    "                'isTemporary': table.isTemporary,\n",
    "                'description': table.description\n",
    "            }\n",
    "            print(f\"  Found table: {table_name} ({table.tableType}) in {table.catalog}.{'.'.join(table.namespace) if table.namespace else 'default'}\")\n",
    "        return table_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting tables from attached lakehouse: {e}\")\n",
    "        return []\n",
    "\n",
    "def determine_tables_to_process(tables_to_process, tables_to_skip):\n",
    "    global table_metadata_cache\n",
    "    all_available_tables = get_all_tables_in_lakehouse_attached()\n",
    "    if not all_available_tables:\n",
    "        print(\"❌ No tables found in attached lakehouse!\")\n",
    "        return []\n",
    "    # print(f\"\\nAvailable tables: {all_available_tables}\")\n",
    "    if tables_to_process == \"ALL\":\n",
    "        selected_tables = all_available_tables\n",
    "    elif isinstance(tables_to_process, list):\n",
    "        selected_tables = []\n",
    "        for table in tables_to_process:\n",
    "            if table in all_available_tables:\n",
    "                selected_tables.append(table)\n",
    "                # print(f\"✓ Table '{table}' found and will be processed\")\n",
    "            else:\n",
    "                print(f\"❌ Table '{table}' not found in attached lakehouse\")\n",
    "        if not selected_tables:\n",
    "            print(\"❌ None of the specified tables were found!\")\n",
    "            return []\n",
    "    elif isinstance(tables_to_process, str):\n",
    "        if tables_to_process in all_available_tables:\n",
    "            selected_tables = [tables_to_process]\n",
    "            # print(f\"✓ Table '{tables_to_process}' found and will be processed\")\n",
    "        else:\n",
    "            print(f\"❌ Table '{tables_to_process}' not found in attached lakehouse\")\n",
    "            return []\n",
    "    else:\n",
    "        print(\"❌ Invalid tables_to_process configuration\")\n",
    "        return []\n",
    "    if tables_to_skip:\n",
    "        selected_tables = [t for t in selected_tables if t not in tables_to_skip]\n",
    "        print(f\"Tables after applying skip list: {selected_tables}\")\n",
    "    tables_before_view_filter = selected_tables.copy()\n",
    "    selected_tables = []\n",
    "    view_tables_skipped = []\n",
    "    for table_name in tables_before_view_filter:\n",
    "        table_metadata = table_metadata_cache.get(table_name)\n",
    "        if table_metadata and table_metadata.get('tableType') == 'VIEW':\n",
    "            view_tables_skipped.append(table_name)\n",
    "            print(f\"⚠ Skipping VIEW table: {table_name}\")\n",
    "        else:\n",
    "            selected_tables.append(table_name)\n",
    "    if view_tables_skipped:\n",
    "        print(f\"Skipped {len(view_tables_skipped)} VIEW tables: {view_tables_skipped}\")\n",
    "    print(f\"Final tables to process: {selected_tables}\")\n",
    "    return selected_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table_attached(table_name):\n",
    "    \"\"\"Process a single table from attached lakehouse and return column descriptions\"\"\"\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    try:\n",
    "        # Load the table using Spark table() method for attached lakehouse\n",
    "        df = spark.table(table_name)\n",
    "        # Get table metadata from cache, or fetch directly if not cached\n",
    "        global table_metadata_cache\n",
    "        table_metadata = table_metadata_cache.get(table_name)\n",
    "        if not table_metadata:\n",
    "            print(f\"  Metadata not in cache, fetching directly for {table_name}\")\n",
    "            try:\n",
    "                tables = spark.catalog.listTables()\n",
    "                for table in tables:\n",
    "                    if table.name == table_name:\n",
    "                        table_metadata = {\n",
    "                            'catalog': table.catalog,\n",
    "                            'namespace': '.'.join(table.namespace) if table.namespace else '',\n",
    "                            'tableType': table.tableType,\n",
    "                            'isTemporary': table.isTemporary,\n",
    "                            'description': table.description\n",
    "                        }\n",
    "                        table_metadata_cache[table_name] = table_metadata\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not fetch metadata for {table_name}: {e}\")\n",
    "        if not table_metadata:\n",
    "            table_metadata = {\n",
    "                'catalog': 'unknown',\n",
    "                'namespace': 'unknown',\n",
    "                'tableType': 'unknown',\n",
    "                'isTemporary': False,\n",
    "                'description': None\n",
    "            }\n",
    "        column_datatypes = get_column_datatypes(df)\n",
    "        columns = df.columns\n",
    "        column_samples = get_column_samples(df)\n",
    "        print(f\"  Analyzing {len(columns)} columns with sample data and datatypes\")\n",
    "        prompt = f\"\"\"You are a metadata documentation assistant for an enterprise data catalog. For the table '{table_name}', you need to:\n",
    "                1. **Entity (Table) Description** — a detailed summary of what data this table has (e.g., employee master data, payroll transactions, job history, etc.), based on the sample data.\n",
    "                2. For each column, generate a description, that describes the values in column, if data is Not confidential data, share a minimum of 4 example values! And assign a sensitivity label!\n",
    "                Sensitivity Labels:\n",
    "                - General: Non-sensitive data like IDs, dates, categories, technical codes\n",
    "                - Confidential: Business data that could impact operations if disclosed\n",
    "                - Highly Confidential: Financial data, proprietary business information, customer data\n",
    "                - HR Confidential: Employee personal information, salary, performance data\n",
    "                Table: {table_name}\n",
    "                Columns with datatypes and sample data:\n",
    "            \"\"\"\n",
    "        for col, samples in column_samples.items():\n",
    "            datatype = column_datatypes.get(col, \"unknown\")\n",
    "            prompt += f\"Column: {col} (DataType: {datatype})\\nSample values: {samples}\\n\"\n",
    "        prompt += \"\"\"\n",
    "            Return ONLY a valid JSON object with the following structure:\n",
    "            {\n",
    "            \"entity_description\": \"A detailed summary of what data this table has, as observed in the sample data.\",\n",
    "            \"columns\": {\n",
    "                \"column_name\": {\n",
    "                \"description\": \"Explain everything observed in the sample data, including patterns, formats, and examples. If data is not confidential, share a minimum of 4 example values in column!\",\n",
    "                \"sensitivity_label\": \"\"\n",
    "                }\n",
    "            }\n",
    "            }\n",
    "\n",
    "            The sensitivity_label must be one of: \"General\", \"Confidential\", \"Highly Confidential\", \"HR Confidential\".\n",
    "            DO NOT INCLUDE ANY EXTRA TEXT, EXPLANATION, MARKDOWN, OR FORMATTING. OUTPUT ONLY VALID JSON OBJECT.\n",
    "            \"\"\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model='gpt-4.1',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a metadata documentation assistant for an enterprise data catalog. I will provide sample records from a table used. Your task is to analyze the data and generate detailed metadata documentation. \"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=8192\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        def extract_json(text):\n",
    "            import re\n",
    "            match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "            return text\n",
    "        def try_fix_json(s):\n",
    "            s = s.strip()\n",
    "            if s.endswith(','):\n",
    "                s = s[:-1]\n",
    "            if s.count('{') > s.count('}') and not s.endswith('}'):\n",
    "                s += '\"}'\n",
    "            elif not s.endswith('}'):\n",
    "                s += '}'\n",
    "            return s\n",
    "        try:\n",
    "            json_content = extract_json(content)\n",
    "            import json\n",
    "            response_data = json.loads(json_content)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                fixed_content = try_fix_json(json_content)\n",
    "                response_data = json.loads(fixed_content)\n",
    "                print(f\"  Warning: JSON was auto-corrected for {table_name}\")\n",
    "            except Exception as e2:\n",
    "                print(f'  Failed to parse response for {table_name}: {e2}')\n",
    "                return []\n",
    "        entity_description = response_data.get('entity_description', f'Table containing data related to {table_name}')\n",
    "        if 'columns' in response_data:\n",
    "            descriptions = response_data['columns']\n",
    "        else:\n",
    "            descriptions = {k: v for k, v in response_data.items() if k != 'entity_description'}\n",
    "            if not entity_description or entity_description == f'Table containing data related to {table_name}':\n",
    "                entity_description = f'Data table with {len(descriptions)} columns including {\", \".join(list(descriptions.keys())[:3])}'\n",
    "        generated_timestamp = get_current_timestamp_ist()\n",
    "        generated_by = \"OpenAI gpt-4.1\"\n",
    "        table_data = []\n",
    "        for col, details in descriptions.items():\n",
    "            if isinstance(details, str):\n",
    "                desc = details\n",
    "                sensitivity = \"General\"\n",
    "            else:\n",
    "                desc = details.get('description', 'No description provided')\n",
    "                sensitivity = details.get('sensitivity_label', 'General')\n",
    "            datatype = column_datatypes.get(col, \"unknown\")\n",
    "            table_data.append({\n",
    "                'WorkspaceName': input_workspace_name,\n",
    "                'WorkspaceId': input_workspace_id,\n",
    "                'LakehouseName': input_lakehouse_name,\n",
    "                'LakehouseId': input_lakehouse_id,\n",
    "                'Entity': table_name,\n",
    "                'EntityDescription': entity_description,\n",
    "                'Catalog': table_metadata.get('catalog', 'unknown'),\n",
    "                'Namespace': table_metadata.get('namespace', 'unknown'),\n",
    "                'TableType': table_metadata.get('tableType', 'unknown'),\n",
    "                'IsTemporary': table_metadata.get('isTemporary', False),\n",
    "                'Attribute': col,\n",
    "                'AttributeDescription': desc,\n",
    "                'DataType': datatype,\n",
    "                'SensitivityLabel': sensitivity,\n",
    "                'GeneratedTimestampIST': generated_timestamp,\n",
    "                'GeneratedBy': generated_by\n",
    "            })\n",
    "        print(f\"  Successfully processed {len(table_data)} columns for {table_name}\")\n",
    "        print(f\"  Entity description: {entity_description[:100]}...\")\n",
    "        print(f\"  Table metadata: {table_metadata['tableType']} in {table_metadata['catalog']}.{table_metadata['namespace']}\")\n",
    "        print(f\"  Generated at: {generated_timestamp}\")\n",
    "        return table_data\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"  ❌ Error processing table {table_name}: {error_msg[:100]}...\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input workspace and lakehouse names automatically\n",
    "print(\"Retrieving input workspace and lakehouse names...\")\n",
    "input_workspace_name, input_lakehouse_name = get_workspace_and_lakehouse_names(input_workspace_id, input_lakehouse_id)\n",
    "print(f\"Input Workspace: {input_workspace_name}\")\n",
    "print(f\"Input Lakehouse: {input_lakehouse_name}\")\n",
    "print(f\"Using attached lakehouse for table discovery\")\n",
    "\n",
    "# Get output workspace and lakehouse names automatically\n",
    "print(\"\\nRetrieving output workspace and lakehouse names...\")\n",
    "output_workspace_name, output_lakehouse_name = get_workspace_and_lakehouse_names(output_workspace_id, output_lakehouse_id)\n",
    "print(f\"Output Workspace: {output_workspace_name}\")\n",
    "print(f\"Output Lakehouse: {output_lakehouse_name}\")\n",
    "print(f\"Output Table: {output_table_name}\")\n",
    "\n",
    "# Show current lakehouse info\n",
    "try:\n",
    "    current_db = spark.catalog.currentDatabase()\n",
    "    print(f\"\\nCurrent Spark database: {current_db}\")\n",
    "except:\n",
    "    print(\"\\nCould not retrieve current Spark database\")\n",
    "\n",
    "output_path = f\"abfss://{output_workspace_id}@onelake.dfs.fabric.microsoft.com/{output_lakehouse_id}/Tables/{output_table_name}\"\n",
    "print(f\"Output path: {output_path}\")\n",
    "\n",
    "# Main execution\n",
    "print(\"\\n=== OpenAI Table Analysis Started (Attached Lakehouse Mode ONLY) ===\")\n",
    "\n",
    "# Get all available tables first for comprehensive reporting\n",
    "print(\"\\n=== Table Discovery and Processing Summary ===\")\n",
    "all_available_tables = get_all_tables_in_lakehouse_attached()\n",
    "\n",
    "if not all_available_tables:\n",
    "    print(\"❌ No tables found in attached lakehouse!\")\n",
    "else:\n",
    "    print(f\"\\n📊 Table Discovery Summary:\")\n",
    "    print(f\"Total tables found in lakehouse: {len(all_available_tables)}\")\n",
    "    print(f\"Available tables: {sorted(all_available_tables)}\")\n",
    "    \n",
    "    # Show configuration\n",
    "    print(f\"\\n⚙️ Processing Configuration:\")\n",
    "    if tables_to_process == \"ALL\":\n",
    "        print(f\"  • Target tables: ALL TABLES\")\n",
    "    elif isinstance(tables_to_process, list):\n",
    "        print(f\"  • Target tables: {tables_to_process}\")\n",
    "    else:\n",
    "        print(f\"  • Target tables: {tables_to_process}\")\n",
    "    \n",
    "    if tables_to_skip:\n",
    "        print(f\"  • Tables to skip: {tables_to_skip}\")\n",
    "    else:\n",
    "        print(f\"  • Tables to skip: None\")\n",
    "\n",
    "# Determine which tables to process\n",
    "tables = determine_tables_to_process(tables_to_process, tables_to_skip)\n",
    "\n",
    "# Enhanced reporting of table processing decisions\n",
    "if all_available_tables:\n",
    "    print(f\"\\n📋 Detailed Table Processing Report:\")\n",
    "    \n",
    "    # Tables that will be processed\n",
    "    if tables:\n",
    "        print(f\"\\n✅ Tables TO BE PROCESSED ({len(tables)}):\")\n",
    "        for i, table_name in enumerate(sorted(tables), 1):\n",
    "            table_metadata = table_metadata_cache.get(table_name, {})\n",
    "            table_type = table_metadata.get('tableType', 'unknown')\n",
    "            catalog = table_metadata.get('catalog', 'unknown')\n",
    "            namespace = table_metadata.get('namespace', 'unknown')\n",
    "            print(f\"  {i:2d}. {table_name} ({table_type}) in {catalog}.{namespace}\")\n",
    "    \n",
    "    # Tables that are skipped due to configuration\n",
    "    skipped_by_config = []\n",
    "    if tables_to_process != \"ALL\":\n",
    "        if isinstance(tables_to_process, list):\n",
    "            skipped_by_config = [t for t in all_available_tables if t not in tables_to_process]\n",
    "        elif isinstance(tables_to_process, str) and tables_to_process != \"ALL\":\n",
    "            skipped_by_config = [t for t in all_available_tables if t != tables_to_process]\n",
    "    \n",
    "    if skipped_by_config:\n",
    "        print(f\"\\n⏭️ Tables SKIPPED (not in target list) ({len(skipped_by_config)}):\")\n",
    "        for i, table_name in enumerate(sorted(skipped_by_config), 1):\n",
    "            table_metadata = table_metadata_cache.get(table_name, {})\n",
    "            table_type = table_metadata.get('tableType', 'unknown')\n",
    "            catalog = table_metadata.get('catalog', 'unknown')\n",
    "            namespace = table_metadata.get('namespace', 'unknown')\n",
    "            print(f\"  {i:2d}. {table_name} ({table_type}) in {catalog}.{namespace}\")\n",
    "    \n",
    "    # Tables explicitly skipped\n",
    "    explicitly_skipped = [t for t in all_available_tables if t in tables_to_skip]\n",
    "    if explicitly_skipped:\n",
    "        print(f\"\\n🚫 Tables EXPLICITLY SKIPPED ({len(explicitly_skipped)}):\")\n",
    "        for i, table_name in enumerate(sorted(explicitly_skipped), 1):\n",
    "            table_metadata = table_metadata_cache.get(table_name, {})\n",
    "            table_type = table_metadata.get('tableType', 'unknown')\n",
    "            catalog = table_metadata.get('catalog', 'unknown')\n",
    "            namespace = table_metadata.get('namespace', 'unknown')\n",
    "            print(f\"  {i:2d}. {table_name} ({table_type}) in {catalog}.{namespace} - Reason: In skip list\")\n",
    "    \n",
    "    # Tables skipped because they are views\n",
    "    view_tables = [t for t in all_available_tables if table_metadata_cache.get(t, {}).get('tableType') == 'VIEW']\n",
    "    if view_tables:\n",
    "        print(f\"\\n👁️ Tables SKIPPED (VIEW type) ({len(view_tables)}):\")\n",
    "        for i, table_name in enumerate(sorted(view_tables), 1):\n",
    "            table_metadata = table_metadata_cache.get(table_name, {})\n",
    "            catalog = table_metadata.get('catalog', 'unknown')\n",
    "            namespace = table_metadata.get('namespace', 'unknown')\n",
    "            print(f\"  {i:2d}. {table_name} (VIEW) in {catalog}.{namespace} - Reason: Views are not processed\")\n",
    "    \n",
    "    # Tables that were requested but not found\n",
    "    if isinstance(tables_to_process, list):\n",
    "        not_found_tables = [t for t in tables_to_process if t not in all_available_tables]\n",
    "        if not_found_tables:\n",
    "            print(f\"\\n❌ Tables REQUESTED but NOT FOUND ({len(not_found_tables)}):\")\n",
    "            for i, table_name in enumerate(sorted(not_found_tables), 1):\n",
    "                print(f\"  {i:2d}. {table_name} - Reason: Table does not exist in lakehouse\")\n",
    "    elif isinstance(tables_to_process, str) and tables_to_process != \"ALL\":\n",
    "        if tables_to_process not in all_available_tables:\n",
    "            print(f\"\\n❌ Table REQUESTED but NOT FOUND:\")\n",
    "            print(f\"  1. {tables_to_process} - Reason: Table does not exist in lakehouse\")\n",
    "\n",
    "if not tables:\n",
    "    print(\"\\n❌ No tables to process! Check your lakehouse attachment and table availability.\")\n",
    "    if all_available_tables:\n",
    "        print(f\"\\n💡 Suggestions:\")\n",
    "        print(f\"  • Available tables you could process: {sorted(all_available_tables)}\")\n",
    "        print(f\"  • Update tables_to_process to include one of these tables\")\n",
    "        print(f\"  • Or set tables_to_process = 'ALL' to process all tables\")\n",
    "else:\n",
    "    # Process all tables using ONLY attached lakehouse method\n",
    "    print(f\"\\n🚀 Starting processing of {len(tables)} table(s)...\")\n",
    "    all_results = []\n",
    "    processed_successfully = []\n",
    "    failed_tables = []\n",
    "    empty_tables = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, table_name in enumerate(tables, 1):\n",
    "        print(f\"\\n[{i}/{len(tables)}] Processing table: {table_name}\")\n",
    "        try:\n",
    "            table_results = process_table_attached(table_name)\n",
    "            if table_results:\n",
    "                all_results.extend(table_results)\n",
    "                processed_successfully.append(table_name)\n",
    "                print(f\"  ✓ Added {len(table_results)} columns from {table_name}\")\n",
    "            else:\n",
    "                empty_tables.append(table_name)\n",
    "                print(f\"  ⚠ No results for {table_name} - table may be empty or inaccessible\")\n",
    "        except Exception as e:\n",
    "            failed_tables.append((table_name, str(e)))\n",
    "            print(f\"  ❌ Failed to process {table_name}: {str(e)}\")\n",
    "        if i < len(tables):\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Final processing summary\n",
    "    print(f\"\\n=== Final Processing Summary ===\")\n",
    "    print(f\"Processing time: {time.time() - start_time:.1f} seconds\")\n",
    "    print(f\"Total tables attempted: {len(tables)}\")\n",
    "    print(f\"Successfully processed: {len(processed_successfully)}\")\n",
    "    print(f\"Empty/inaccessible tables: {len(empty_tables)}\")\n",
    "    print(f\"Failed tables: {len(failed_tables)}\")\n",
    "    \n",
    "    if processed_successfully:\n",
    "        print(f\"\\n✅ Successfully processed tables ({len(processed_successfully)}):\")\n",
    "        for table_name in sorted(processed_successfully):\n",
    "            table_columns = len([r for r in all_results if r['Entity'] == table_name])\n",
    "            print(f\"  • {table_name}: {table_columns} columns\")\n",
    "    \n",
    "    if empty_tables:\n",
    "        print(f\"\\n⚠️ Empty/inaccessible tables ({len(empty_tables)}):\")\n",
    "        for table_name in sorted(empty_tables):\n",
    "            print(f\"  • {table_name}\")\n",
    "    \n",
    "    if failed_tables:\n",
    "        print(f\"\\n❌ Failed tables ({len(failed_tables)}):\")\n",
    "        for table_name, error in failed_tables:\n",
    "            print(f\"  • {table_name}: {error[:100]}...\")\n",
    "    \n",
    "    if all_results:\n",
    "        print(f\"\\n=== Saving Results ===\")\n",
    "        print(f\"Total columns processed: {len(all_results)}\")\n",
    "        print(f\"Tables with data: {len(set([r['Entity'] for r in all_results]))}\")\n",
    "        \n",
    "        # Convert to Spark DataFrame for saving\n",
    "        spark_result_df = spark.createDataFrame(all_results)\n",
    "        \n",
    "        # Save to Delta table in the specified output lakehouse\n",
    "        print(f\"\\nSaving results to output location...\")\n",
    "        print(f\"Output Workspace: {output_workspace_name}\")\n",
    "        print(f\"Output Lakehouse: {output_lakehouse_name}\")\n",
    "        print(f\"Output Table: {output_table_name}\")\n",
    "        \n",
    "        try:\n",
    "            notebookutils.fs.rm(output_path, True)\n",
    "            print(f\"Existing table dropped: {output_path}\")\n",
    "        except:\n",
    "            print(f\"No existing table to drop at: {output_path}\")\n",
    "        \n",
    "        spark_result_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        \n",
    "        # Save results as a single CSV file\n",
    "        print(f\"Saving results as a single CSV file...\")\n",
    "        \n",
    "        # Parse the CSV path to separate directory and filename\n",
    "        import os\n",
    "        csv_directory = os.path.dirname(output_csv_full_path)\n",
    "        csv_filename = os.path.basename(output_csv_full_path)\n",
    "        \n",
    "        print(f\"CSV Directory: {csv_directory}\")\n",
    "        print(f\"CSV Filename: {csv_filename}\")\n",
    "        \n",
    "        # First, remove any existing CSV file\n",
    "        try:\n",
    "            notebookutils.fs.rm(output_csv_full_path, True)\n",
    "            print(f\"Existing CSV file removed: {output_csv_full_path}\")\n",
    "        except:\n",
    "            print(f\"No existing CSV file to remove at: {output_csv_full_path}\")\n",
    "        \n",
    "        # Create a temporary directory for Spark to write to\n",
    "        temp_csv_dir = f\"{csv_directory}/temp_csv_output\"\n",
    "        \n",
    "        try:\n",
    "            notebookutils.fs.rm(temp_csv_dir, True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Coalesce to a single partition and write as CSV to temp directory\n",
    "        single_partition_df = spark_result_df.coalesce(1)\n",
    "        single_partition_df.write.mode(\"overwrite\").option(\"header\", True).csv(temp_csv_dir)\n",
    "        \n",
    "        # Find the actual CSV file (Spark creates a part-xxxxx.csv file)\n",
    "        temp_csv_files = notebookutils.fs.ls(temp_csv_dir)\n",
    "        part_csv_file = None\n",
    "        for file in temp_csv_files:\n",
    "            if file.name.startswith(\"part-\") and file.name.endswith(\".csv\"):\n",
    "                part_csv_file = file.path\n",
    "                break\n",
    "        \n",
    "        if part_csv_file:\n",
    "            # Move the part file to the final location with the specified filename\n",
    "            try:\n",
    "                notebookutils.fs.mv(part_csv_file, output_csv_full_path)\n",
    "                print(f\"✓ Results saved as single CSV file: {output_csv_full_path}\")\n",
    "                \n",
    "                # Clean up temporary directory\n",
    "                try:\n",
    "                    notebookutils.fs.rm(temp_csv_dir, True)\n",
    "                    print(f\"Temporary directory cleaned up: {temp_csv_dir}\")\n",
    "                except:\n",
    "                    print(f\"Note: Could not clean up temporary directory: {temp_csv_dir}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error moving CSV file: {e}\")\n",
    "                print(f\"CSV file is available at: {part_csv_file}\")\n",
    "        else:\n",
    "            print(f\"Error: Could not find the generated CSV file in: {temp_csv_dir}\")\n",
    "            print(f\"Files in temp directory: {[f.name for f in temp_csv_files]}\")\n",
    "        \n",
    "        print(\"\\nEntities processed:\")\n",
    "        entities = [row['Entity'] for row in spark_result_df.select('Entity').distinct().collect()]\n",
    "        for entity in sorted(entities):\n",
    "            entity_data = spark_result_df.filter(spark_result_df['Entity'] == entity)\n",
    "            count = entity_data.count()\n",
    "            datatypes = entity_data.select('Datatype').distinct().count()\n",
    "            table_type = entity_data.select('TableType').first()['TableType'] if count > 0 else 'unknown'\n",
    "            catalog = entity_data.select('Catalog').first()['Catalog'] if count > 0 else 'unknown'\n",
    "            namespace = entity_data.select('Namespace').first()['Namespace'] if count > 0 else 'unknown'\n",
    "            is_temporary = entity_data.select('IsTemporary').first()['IsTemporary'] if count > 0 else 'unknown'\n",
    "            print(f\"  {entity} ({table_type}): {count} columns, {datatypes} unique datatypes\")\n",
    "            print(f\"    Location: {catalog}.{namespace}, Temporary: {is_temporary}\")\n",
    "    else:\n",
    "        print(\"\\n❌ No results generated - check table names and data availability in attached lakehouse\")\n",
    "        print(\"Ensure your lakehouse is properly attached and contains accessible tables\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
